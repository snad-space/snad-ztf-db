server {
  listen      8001 default_server;
  server_name sai.db.ztf.snad.space;
  location / {
    proxy_pass       https://sai.db.ztf.snad.space:443;
    proxy_set_header Host sai.db.ztf.snad.space:443;
  }
}

server {
   listen      8002 default_server;
   server_name uci.db.ztf.snad.space;
   location / {
     proxy_pass       https://uci.db.ztf.snad.space:443;
     proxy_set_header Host uci.db.ztf.snad.space:443;
   }
}

server {
   listen      8003 default_server;
   server_name lpc.db.ztf.snad.space;
   location / {
     proxy_pass       https://lpc.db.ztf.snad.space:443;
     proxy_set_header Host lpc.db.ztf.snad.space:443;
   }
}

upstream main {
  server 127.0.0.1:8003;
  server 127.0.0.1:8001 backup;
  server 127.0.0.2:8002 backup;
}

lua_shared_dict healthcheck 1m;

lua_socket_log_errors off;

init_worker_by_lua_block {
    local hc = require "resty.upstream.healthcheck"

    local ok, err = hc.spawn_checker{
        shm = "healthcheck",  -- defined by "lua_shared_dict"
        upstream = "main", -- defined by "upstream"
        type = "http", -- support "http" and "https"

        http_req = "GET / HTTP/1.0\r\nHost: main \r\n\r\n",
                -- raw HTTP request for checking

        port = nil,  -- the check port, it can be different than the original backend server port, default means the same as the original backend server
        interval = 5000,  -- run the check cycle every 2 sec
        timeout = 5000,   -- 1 sec is the timeout for network operations
        fall = 3,  -- # of successive failures before turning a peer down
        rise = 2,  -- # of successive successes before turning a peer up
        valid_statuses = {200},  -- a list valid HTTP status code
        concurrency = 10,  -- concurrency level for test requests
    }
    if not ok then
        ngx.log(ngx.ERR, "failed to spawn health checker: ", err)
        return
    end

    -- Just call hc.spawn_checker() for more times here if you have
    -- more upstream groups to monitor. One call for one upstream group.
    -- They can all share the same shm zone without conflicts but they
    -- need a bigger shm zone for obvious reasons.
}

server {
  listen      80;
  server_name db.ztf.snad.space;
  location / {
    proxy_pass http://main;
    proxy_next_upstream error timeout invalid_header http_500 http_502 http_503 http_504 http_429;
  }

  # status page for all the peers:
  location = /status {
    access_log off;

    default_type text/plain;
      content_by_lua_block {
          local hc = require "resty.upstream.healthcheck"
          ngx.say("Nginx Worker PID: ", ngx.worker.pid())
          ngx.print(hc.status_page())
      }
  }
}
